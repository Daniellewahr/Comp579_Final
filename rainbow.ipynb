{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Rainbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from DQN_CNN import Net, train_game, eval_game, one_hot_encode_game_state, epsilon_greedy_action # Import necessary components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step, add Double Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_game_double_q(game, it, batch_size, gamma, optimizer, criterion, device, model, target_model):\n",
    "    global losses\n",
    "    global scores\n",
    "    batch_outputs = []\n",
    "    batch_labels = []\n",
    "    step = 1\n",
    "\n",
    "    while not game.game_over():\n",
    "        state = one_hot_encode_game_state(game.state())\n",
    "        state_tensor = state.unsqueeze(0).permute(0, 3, 1, 2).to(device)\n",
    "\n",
    "        Q_values = model(state_tensor)\n",
    "        Q_valid_values = [Q_values[0][a] if game.is_action_available(a) else float('-inf') for a in range(4)]\n",
    "        action = epsilon_greedy_action(np.array(Q_valid_values))\n",
    "        reward = game.do_action(action)\n",
    "\n",
    "        new_state = game.state()\n",
    "        new_state_tensor = one_hot_encode_game_state(new_state).unsqueeze(0).permute(0, 3, 1, 2).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Q_next = target_model(new_state_tensor)  # Use target model for stability\n",
    "            Q_next_policy = model(new_state_tensor)  # Use policy model to select action\n",
    "\n",
    "        # Double Q-Learning update rule\n",
    "        next_action = torch.argmax(Q_next_policy).item()\n",
    "        target_Q_value = reward + gamma * Q_next[0][next_action]\n",
    "\n",
    "        batch_outputs.append(Q_values[0][action])\n",
    "        batch_labels.append(target_Q_value)\n",
    "\n",
    "        if step % batch_size == 0 or game.game_over():\n",
    "            if len(batch_labels) == 0: return\n",
    "            optimizer.zero_grad()\n",
    "            label_tensor = torch.tensor(batch_labels, dtype=torch.float32).to(device)\n",
    "            output_tensor = torch.stack(batch_outputs).to(device)\n",
    "            batch_labels, batch_outputs = [], []\n",
    "            loss = criterion(output_tensor, label_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if game.game_over():\n",
    "                scores.append(game.score())\n",
    "                if it % 100 == 0 and it > 0:\n",
    "                    mean_score = sum(scores[-100:]) / 100\n",
    "                    print(f\"Epoch: {it}, Mean score last 100 epochs: {mean_score:.2f}\")\n",
    "                return\n",
    "        step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Device is cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from DQN_CNN import Net, Game, one_hot_encode_game_state  # Make sure to import your game environment and necessary functions\n",
    "\n",
    "# Set the parameters as specified\n",
    "input_shape = (16, 4, 4)  # Shape of the game state, assuming one-hot encoded\n",
    "num_actions = 4  # Number of possible actions in the game\n",
    "batch_size = 128  # Batch size for training\n",
    "gamma = 1  # Discount factor for future rewards\n",
    "n_epoch = 1000\n",
    "n_eval = 100  # Number of games to evaluate\n",
    "SEED = 1  # Seed for reproducibility\n",
    "\n",
    "# Set seeds for random number generators to ensure reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior in CuDNN\n",
    "\n",
    "# Determine if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the neural network model\n",
    "model = Net(input_shape, num_actions).to(device)\n",
    "\n",
    "# Set the learning rate for optimization\n",
    "learning_rate = 0.0001\n",
    "# Initialize the optimizer with the model parameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss().to(device)\n",
    "\n",
    "# Initialize lists to store losses and scores during training\n",
    "losses = []\n",
    "scores = []\n",
    "\n",
    "# Initialize the game environment\n",
    "game = Game()  # Make sure the Game class has methods like game_over(), state(), do_action(), and score()\n",
    "\n",
    "# Print to confirm everything is set up correctly\n",
    "print(\"Setup complete. Device is\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Mean score last 100 epochs: 1094.16\n",
      "Epoch: 200, Mean score last 100 epochs: 1129.08\n",
      "Epoch: 300, Mean score last 100 epochs: 1129.52\n",
      "Epoch: 400, Mean score last 100 epochs: 1163.40\n",
      "Epoch: 500, Mean score last 100 epochs: 1076.12\n",
      "Epoch: 600, Mean score last 100 epochs: 1056.32\n",
      "Epoch: 700, Mean score last 100 epochs: 1017.20\n",
      "Epoch: 800, Mean score last 100 epochs: 1061.60\n",
      "Epoch: 900, Mean score last 100 epochs: 1080.56\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# Initialize models\n",
    "model = Net(input_shape, num_actions).to(device)\n",
    "target_model = deepcopy(model)  # Create a target model as a deep copy of the model\n",
    "\n",
    "# Training loop\n",
    "for it in range(n_epoch):\n",
    "    game = Game()\n",
    "    train_game_double_q(game, it, batch_size, gamma, optimizer, criterion, device, model, target_model)\n",
    "    if it % 10 == 0:  # Update the target network every 10 iterations\n",
    "        target_model.load_state_dict(model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, n_games, device):\n",
    "    total_score = 0\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    for _ in range(n_games):\n",
    "        game = Game()  # Initialize a new game\n",
    "        while not game.game_over():\n",
    "            state = one_hot_encode_game_state(game.state())  # Get the current state and encode it\n",
    "            state_tensor = state.unsqueeze(0).permute(0, 3, 1, 2).to(device)  # Reshape and move to device\n",
    "\n",
    "            with torch.no_grad():  # Ensure no gradients are computed during inference\n",
    "                Q_values = model(state_tensor)\n",
    "                Q_valid_values = [Q_values[0][a] if game.is_action_available(a) else float('-inf') for a in range(num_actions)]\n",
    "                best_action = np.argmax(Q_valid_values)  # Choose the action with the highest Q-value\n",
    "\n",
    "            game.do_action(best_action)  # Perform the action in the game\n",
    "\n",
    "        total_score += game.score()  # Accumulate the score from the finished game\n",
    "\n",
    "    mean_score = total_score / n_games  # Calculate the average score across all games\n",
    "    print(f\"Average score over {n_games} games: {mean_score}\")\n",
    "    return mean_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluating model after just adding Double to base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score over 100 games: 1156.64\n",
      "mean score of model: 1156.64\n"
     ]
    }
   ],
   "source": [
    "# Example of using the evaluate_model function\n",
    "n_eval_games = 100  # Set the number of games for evaluation\n",
    "mean_score = evaluate_model(model, n_eval_games, device)\n",
    "print(f\"mean score of model: {mean_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding PER on top of Double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.alpha = alpha\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.position = 0\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.priorities[self.position] = max_prio\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.position]\n",
    "        \n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights = np.array(weights, dtype=np.float32)\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones), indices, weights\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for idx, prio in zip(indices, priorities):\n",
    "            self.priorities[idx] = prio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_game_with_double_q_per(game, it, batch_size, gamma, optimizer, criterion, device, model, target_model, buffer, beta):\n",
    "    global losses\n",
    "    global scores\n",
    "    state = Game()\n",
    "    state = one_hot_encode_game_state(state).unsqueeze(0).permute(0, 3, 1, 2).to(device)\n",
    "    while not game.game_over():\n",
    "        with torch.no_grad():\n",
    "            Q_values = model(state)\n",
    "            Q_valid_values = [Q_values[0][a] if game.is_action_available(a) else float('-inf') for a in range(4)]\n",
    "            action = epsilon_greedy_action(np.array(Q_valid_values))\n",
    "        \n",
    "        reward = game.do_action(action)\n",
    "        next_state = game.state()\n",
    "        done = game.game_over()\n",
    "        next_state = one_hot_encode_game_state(next_state).unsqueeze(0).permute(0, 3, 1, 2).to(device)\n",
    "\n",
    "        buffer.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        if len(buffer) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones, indices, weights = buffer.sample(batch_size, beta)\n",
    "            states = torch.tensor(states).float().to(device)\n",
    "            next_states = torch.tensor(next_states).float().to(device)\n",
    "            actions = torch.tensor(actions).long().to(device)\n",
    "            rewards = torch.tensor(rewards).float().to(device)\n",
    "            dones = torch.tensor(dones).float().to(device)\n",
    "            weights = torch.tensor(weights).float().to(device)\n",
    "\n",
    "            current_q = model(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "            with torch.no_grad():\n",
    "                # Using the target model to calculate the next Q-values for stability\n",
    "                next_q_values = target_model(next_states)\n",
    "                next_q_values_policy = model(next_states)\n",
    "                next_actions = next_q_values_policy.max(1)[1]\n",
    "                next_q = next_q_values.gather(1, next_actions.unsqueeze(-1)).squeeze(-1)\n",
    "                \n",
    "                expected_q = rewards + gamma * next_q * (1 - dones)\n",
    "\n",
    "            loss = (current_q - expected_q.detach()).pow(2) * weights\n",
    "            prios = loss + 1e-5\n",
    "            loss = loss.mean()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            buffer.update_priorities(indices, prios.cpu().numpy())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    if game.game_over():\n",
    "        scores.append(game.score())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Game' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m it \u001b[38;5;241m%\u001b[39m update_target_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     10\u001b[0m     target_model\u001b[38;5;241m.\u001b[39mload_state_dict(model\u001b[38;5;241m.\u001b[39mstate_dict())\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrain_game_with_double_q_per\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_start\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m, in \u001b[0;36mtrain_game_with_double_q_per\u001b[0;34m(game, it, batch_size, gamma, optimizer, criterion, device, model, target_model, buffer, beta)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m scores\n\u001b[1;32m      4\u001b[0m state \u001b[38;5;241m=\u001b[39m Game()\n\u001b[0;32m----> 5\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mone_hot_encode_game_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m game\u001b[38;5;241m.\u001b[39mgame_over():\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/danny_version_COMP579_FINAL/Comp579_Final/DQN_CNN.py:21\u001b[0m, in \u001b[0;36mone_hot_encode_game_state\u001b[0;34m(game_state)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mConvert a 4x4 matrix of integers with values 1-16 to a 4x4x16 matrix of one-hot encoded vectors.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03mtorch.Tensor: a 4x4x16 tensor of one-hot encoded vectors.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Initialize a tensor of zeros with the shape 4x4x16\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m one_hot_encoded_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[43mgame_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m16\u001b[39m,))\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Populate the tensor with one-hot encoding\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(game_state\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Game' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "target_model = deepcopy(model)\n",
    "update_target_every = 10  # Number of episodes after which to update the target network\n",
    "buffer_capacity = 10000  # Choose a size that fits your game and system memory\n",
    "beta_start = 0.4  # Importance sampling weight\n",
    "beta_frames = 1000  # Total number of frames to reach beta=1\n",
    "buffer = PrioritizedReplayBuffer(capacity=buffer_capacity)\n",
    "\n",
    "for it in range(n_epoch):\n",
    "    if it % update_target_every == 0:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "    train_game_with_double_q_per(game, it, batch_size, gamma, optimizer, criterion, device, model, target_model, buffer, beta_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score over 100 games: 1113.12\n",
      "mean score of model: 1113.12\n"
     ]
    }
   ],
   "source": [
    "# Example of using the evaluate_model function\n",
    "n_eval_games = 100  # Set the number of games for evaluation\n",
    "mean_score = evaluate_model(model, n_eval_games, device)\n",
    "print(f\"mean score of model: {mean_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
